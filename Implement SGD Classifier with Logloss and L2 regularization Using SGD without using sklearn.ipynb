{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Assignment 9 : Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "L8W2fg1cyGdX",
    "outputId": "d00484db-184c-4799-9fe7-eae9e8a63126"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "gONY1YiDq7jD"
   },
   "outputs": [],
   "source": [
    "# Standardizing the data.\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "x_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0DR_YMBsyOci",
    "outputId": "a9cba0bf-a377-4eb6-db64-fdd3b61a3d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "3HpvTwDHyQQy",
    "outputId": "6def3a27-2e4a-4032-f1f9-32188826261b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "id": "YYaVyQ2lyXcr",
    "outputId": "af0b141f-4d0e-46b6-811c-af129062936c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.77, NNZs: 15, Bias: -0.316653, T: 37500, Avg. loss: 0.455552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.472747, T: 75000, Avg. loss: 0.394686\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580082, T: 112500, Avg. loss: 0.385711\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.658292, T: 150000, Avg. loss: 0.382083\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719528, T: 187500, Avg. loss: 0.380486\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.763409, T: 225000, Avg. loss: 0.379578\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.795106, T: 262500, Avg. loss: 0.379150\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.819925, T: 300000, Avg. loss: 0.378856\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.837805, T: 337500, Avg. loss: 0.378585\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.853138, T: 375000, Avg. loss: 0.378630\n",
      "Total training time: 0.09 seconds.\n",
      "Convergence after 10 epochs took 0.09 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "EAfkVI6GyaRO",
    "outputId": "16972c45-b824-4f11-ccb6-5c3330d511b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42336692,  0.18547565, -0.14859036,  0.34144407, -0.2081867 ,\n",
       "          0.56016579, -0.45242483, -0.09408813,  0.2092732 ,  0.18084126,\n",
       "          0.19705191,  0.00421916, -0.0796037 ,  0.33852802,  0.02266721]]),\n",
       " (1, 15),\n",
       " array([-0.8531383]))"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "  \n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "\n",
    "    w = np.zeros_like(dim, dtype = float)\n",
    "\n",
    "    b = 0\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "A7I6uWBRsKc4",
    "outputId": "30b4b29e-120f-4021-f6b6-cee786b0e49a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = X_train[0] \n",
    "w, b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Pv1llH429wG5",
    "outputId": "b8859d75-ccff-451c-ddab-7a48edd7ab76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "\n",
    "def grader_weights(w, b):\n",
    "  assert((len(w) == len(dim)) and b == 0 and np.sum(w) == 0.0)\n",
    "  return True\n",
    "\n",
    "grader_weights(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  \n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "\n",
    "    exp_Val = np.exp(-z)\n",
    "    Denominator = np.add(1, exp_Val)\n",
    "    Sigmoid_Val = np.divide(1, Denominator)\n",
    "\n",
    "    return Sigmoid_Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "P_JASp_NAfK_",
    "outputId": "ce12b418-6d16-4ba7-f919-4bbb16903766"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val = sigmoid(z)\n",
    "  assert(val == 0.8807970779778823)\n",
    "  return True\n",
    "  \n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "\n",
    "    '''In this function, we will compute log loss '''\n",
    "    \n",
    "    Entropy = 0.0\n",
    "    for y, y_hat in zip(y_true, y_pred):\n",
    "      log_10_t = np.where(y_hat > 0.0000000001, y_hat, 0)\n",
    "      true_Event = np.multiply(y, np.log10(log_10_t, out = log_10_t, where = log_10_t > 0))\n",
    "\n",
    "      log_10_f = np.where((1 - y_hat) > 0.0000000001, (1 - y_hat), 0)\n",
    "      false_Event = np.multiply((1 - y), np.log10(log_10_f, out = log_10_f, where = log_10_f > 0))\n",
    "\n",
    "      Entropy += np.add(true_Event, false_Event)\n",
    "\n",
    "    loss = -np.divide(Entropy, len(y_true))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LzttjvBFCuQ5",
    "outputId": "e66d7f3a-ed28-4cde-badd-8458b197b376"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true, pred):\n",
    "  loss = logloss(true, pred)\n",
    "  assert(loss == 0.07644900402910389)\n",
    "  return True\n",
    "\n",
    "true=[1, 1, 0, 1, 0]\n",
    "pred = [0.9, 0.8, 0.1, 0.8, 0.2]\n",
    "grader_logloss(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x, y, w, b, alpha, N):\n",
    "   \n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    \n",
    "    Z = np.sum(np.add(np.multiply(w, x), b))\n",
    "    Sig_Z = sigmoid(Z)\n",
    "    otmztn_Eq = np.multiply(x, np.subtract(y, Sig_Z))\n",
    "    l2_norm = -np.multiply(np.divide(alpha, N), w)\n",
    "    dw = np.add(otmztn_Eq, l2_norm)\n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "WI3xD8ctGEnJ",
    "outputId": "8abf1636-690a-4a81-e1ed-06c7dfda8ff3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x, y, w, b, alpha, N):\n",
    "  grad_dw = gradient_dw(x, y, w, b, alpha, N)\n",
    "  assert(np.sum(grad_dw) == 2.613689585)\n",
    "  return True\n",
    "grad_x = np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y = 0\n",
    "grad_w, grad_b = initialize_weights(grad_x)\n",
    "alpha = 0.0001\n",
    "N = len(X_train)\n",
    "grader_dw(grad_x, grad_y, grad_w, grad_b, alpha, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x, y, w, b):\n",
    "\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "\n",
    "     Z = np.sum(np.add(np.multiply(w, x), b))\n",
    "     Sig_Z = sigmoid(Z)\n",
    "\n",
    "     otmztn_Eq_i = np.subtract(y, Sig_Z)\n",
    "\n",
    "     db = np.add(otmztn_Eq_i, b)\n",
    "\n",
    "     return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TfFDKmscG5qZ",
    "outputId": "3f6eaf89-7b83-4ff4-f388-2de528f15acd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x, y, w, b):\n",
    "  grad_db = gradient_db(x, y, w, b)\n",
    "  assert(grad_db == -0.5)\n",
    "  return True\n",
    "grad_x = np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y = 0\n",
    "grad_w,grad_b = initialize_weights(grad_x)\n",
    "alpha = 0.0001\n",
    "N = len(X_train)\n",
    "grader_db(grad_x, grad_y, grad_w, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "dmAdc5ejEZ25",
    "outputId": "c3fa8e8f-2776-4cbf-e97a-f0d31290c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "37500\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, y_train, X_test, y_test, epochs, alpha, eta0):\n",
    "\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    train_losses, test_losses = list(), list()\n",
    "\n",
    "    for epoch in tqdm(range(0, epochs)):\n",
    "\n",
    "      for features, y in zip(X_train, y_train):\n",
    "\n",
    "        grad_W = gradient_dw(features, y, w, b, alpha, N)\n",
    "        grad_B = gradient_db(features, y, w, b)\n",
    "        w += (eta0 * grad_W)\n",
    "        b += (eta0 * grad_B)\n",
    "        \n",
    "      y_hat_train, y_hat_test = list(), list()\n",
    "\n",
    "      for train_Features, test_Features in zip(X_train, X_test):\n",
    "\n",
    "        linear_Cal_train = np.float64(np.add(np.multiply(train_Features, w), b))\n",
    "        Sig_Z_train = sigmoid(linear_Cal_train)\n",
    "        y_hat_train.append(Sig_Z_train)\n",
    "\n",
    "        linear_Call_test = np.float64(np.add(np.multiply(test_Features, w), b))\n",
    "        Sig_Z_test = sigmoid(linear_Call_test)\n",
    "        y_hat_test.append(Sig_Z_test)\n",
    "\n",
    "      loss_train, loss_test = 0.0, 0.0\n",
    "\n",
    "      loss_train = logloss(y_train, y_hat_train)\n",
    "      train_losses.append(loss_train)\n",
    "\n",
    "      loss_test = logloss(y_test, y_hat_test)\n",
    "      test_losses.append(loss_test)\n",
    "\n",
    "    return w, b, train_losses, test_losses\n",
    "\n",
    "print(type(X_train))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sUquz7LFEZ6E",
    "outputId": "c101b573-e3c0-41f7-bb2f-c6a4bc6c18b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:08<00:00,  2.57s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.0001\n",
    "eta0 = 0.0001\n",
    "N = len(X_train)\n",
    "epochs = 50\n",
    "w, b, train_losses, test_losses = train(X_train, y_train, X_test, y_test, epochs, alpha, eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "TAfteTYMdwUQ",
    "outputId": "4fded2ea-2fa2-4f66-be94-8ff9958ce91f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1488941164241025e+77"
      ]
     },
     "execution_count": 104,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "kf3mF_ozdwIz",
    "outputId": "16eab073-0006-4fbd-e7ee-33d1ba819c7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.78967093e+01,  3.68155464e+01,  3.85423546e+01,  3.90836987e-01,\n",
       "       -3.82763541e+01,  3.65004322e+01, -3.70102232e+01,  3.73129008e+01,\n",
       "        7.02578975e+01,  1.43333145e+00,  4.14233540e+01, -6.58182248e+01,\n",
       "       -2.16904039e+01,  6.60235392e-02,  4.60944273e+01])"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "nx8Rs9rfEZ1R",
    "outputId": "9cc2f9dd-1342-4aa5-8626-a715760db40f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-3.74733424e+01,  3.66300708e+01,  3.86909450e+01,\n",
       "          4.93929174e-02, -3.80681674e+01,  3.59402665e+01,\n",
       "         -3.65577983e+01,  3.74069889e+01,  7.00486243e+01,\n",
       "          1.25249019e+00,  4.12263021e+01, -6.58224439e+01,\n",
       "         -2.16108002e+01, -2.72504477e-01,  4.60717601e+01]]),\n",
       " array([-4.14889412e+77]))"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 552
    },
    "id": "1O6GrRt7UeCJ",
    "outputId": "0621c09c-c830-4baa-9915-e7d45f85c953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f22d7005e48>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f34cf8>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f34e48>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f34f98>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35128>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35278>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f353c8>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35518>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35668>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f357b8>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f3c4e0>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35a20>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35b70>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35cc0>,\n",
       " <matplotlib.lines.Line2D at 0x7f22d6f35e10>]"
      ]
     },
     "execution_count": 107,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Epoch No. vs Train,Test Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(0, epochs), train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "FUN8puFoEZtU",
    "outputId": "f544dcd1-fc70-410f-ce4b-e4cf4ce3895b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6978933333333333\n",
      "0.6986399999999999\n"
     ]
    }
   ],
   "source": [
    "def pred(w, b, X):\n",
    "\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        z = np.dot(w, X[i]) + b\n",
    "        if sigmoid(z) >= 0.5:\n",
    "          predict.append(1)\n",
    "        else:\n",
    "          predict.append(0)\n",
    "\n",
    "    return np.array(predict)\n",
    "\n",
    "print(1 - np.sum(y_train - pred(w, b, X_train)) / len(X_train))\n",
    "print(1 - np.sum(y_test  - pred(w, b, X_test)) / len(X_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
