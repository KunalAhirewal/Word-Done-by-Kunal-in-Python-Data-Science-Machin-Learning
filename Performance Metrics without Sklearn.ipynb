{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Assignment 7 : Compute Performance Metrics without Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A :    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaFLW7oBQvnt",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  1.0  0.637387\n",
      "1  1.0  0.635165\n",
      "2  1.0  0.766586\n",
      "3  1.0  0.724564\n",
      "4  1.0  0.889199\n",
      "5  1.0  0.601600\n",
      "6  1.0  0.666323\n",
      "7  1.0  0.567012\n",
      "8  1.0  0.650230\n",
      "9  1.0  0.829346\n",
      "\n",
      "\n",
      " 1.0    10000\n",
      "0.0      100\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"5_a.csv\")\n",
    "print(dataframe.head(10))\n",
    "print(\"\\n\\n\", dataframe['y'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrics :\n",
      "[[    0     0]\n",
      " [  100 10000]]\n"
     ]
    }
   ],
   "source": [
    "def TP_TN_FP_FN(dataframe):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    y_hat_new = []\n",
    "    for i in dataframe['proba']:\n",
    "        if(i < 0.5):\n",
    "            y_hat_new.append(0.0)\n",
    "        else:\n",
    "            y_hat_new.append(1.0)\n",
    "    dataframe['y_predict'] = y_hat_new\n",
    "    \n",
    "    TP = ((dataframe['y'] == 1.0) & (dataframe['y_predict'] == 1.0)).sum()\n",
    "    TN = ((dataframe['y'] == 0.0) & (dataframe['y_predict'] == 0.0)).sum()\n",
    "    FP = ((dataframe['y'] == 0.0) & (dataframe['y_predict'] == 1.0)).sum()\n",
    "    FN = ((dataframe['y'] == 1.0) & (dataframe['y_predict'] == 0.0)).sum()\n",
    "    \n",
    "    return (TP, TN, FP, FN)\n",
    "\n",
    "TP, TN, FP, FN = TP_TN_FP_FN(dataframe)\n",
    "\n",
    "def ConfusionMetrics(TP, TN, FP, FN):\n",
    "    \n",
    "    print(\"Confusion Matrics :\")    \n",
    "    print(np.array([[TN, FN], [FP, TP]]))\n",
    "    \n",
    "ConfusionMetrics(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compute_F1_Score(TP, TN, FP, FN):\n",
    "    \n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    F1_Score = (2 * (Precision * Recall) / (Precision + Recall))\n",
    "    \n",
    "    print(\"F1 Score : \", F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.9950248756218906\n"
     ]
    }
   ],
   "source": [
    "Compute_F1_Score(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.48829900000000004\n"
     ]
    }
   ],
   "source": [
    "def Compute_AUC_Score(dataframe):\n",
    "    TPR=[]\n",
    "    FPR=[]\n",
    "    Prob_data = list(dataframe.proba.unique())\n",
    "    Prob_data.sort(reverse = True)\n",
    "    for tow in Prob_data:\n",
    "        y_hat_new = []\n",
    "        for i in dataframe['proba']:\n",
    "            if (i <  tow):\n",
    "                y_hat_new.append(0)\n",
    "            else :\n",
    "                y_hat_new.append(1)\n",
    "           \n",
    "        dataframe['y_predict'] = y_hat_new\n",
    "   \n",
    "        TN = (((dataframe['y'])==0) & ((dataframe['y_predict'])==0)).sum()\n",
    "        TP = (((dataframe['y'])==1) & ((dataframe['y_predict'])==1)).sum()\n",
    "        FP = (((dataframe['y'])==0) & ((dataframe['y_predict'])==1)).sum()\n",
    "        FN = (((dataframe['y'])==1) & ((dataframe['y_predict'])==0)).sum()\n",
    "       \n",
    "        TPR.append(TP/(TP+FN))\n",
    "        FPR.append(FP/(FP+TN))\n",
    "\n",
    "    A = sorted(TPR)\n",
    "    B = sorted(FPR)\n",
    "    AUC = np.trapz(A, B)\n",
    "   \n",
    "    print('AUC Score :',AUC)\n",
    "    \n",
    "Compute_AUC_Score(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9900990099009901\n"
     ]
    }
   ],
   "source": [
    "def Compute_Accuracy_Score(dataframe):\n",
    "    Count = 0\n",
    "    for i in range(len(dataframe)):\n",
    "        if dataframe['y'][i] == dataframe['y_predict'][i]:\n",
    "            Count +=1\n",
    "    \n",
    "    total = len(dataframe)\n",
    "    Accuracy_Score = Count / total\n",
    "    print(\"Accuracy: \", Accuracy_Score)\n",
    "\n",
    "Compute_Accuracy_Score(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     y     proba\n",
      "0  0.0  0.281035\n",
      "1  0.0  0.465152\n",
      "2  0.0  0.352793\n",
      "3  0.0  0.157818\n",
      "4  0.0  0.276648\n",
      "5  0.0  0.190260\n",
      "6  0.0  0.320328\n",
      "7  0.0  0.435013\n",
      "8  0.0  0.284849\n",
      "9  0.0  0.427919\n",
      "\n",
      "\n",
      " 0.0    10000\n",
      "1.0      100\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataframe1 = pd.read_csv(\"5_b.csv\")\n",
    "print(dataframe1.head(10))\n",
    "print(\"\\n\\n\", dataframe1['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2sKlq0YQvn5"
   },
   "source": [
    "### 1. Compute Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Metrics : \n",
      "[[9761   45]\n",
      " [ 239   55]]\n"
     ]
    }
   ],
   "source": [
    "TP, TN, FP, FN = TP_TN_FP_FN(dataframe1)\n",
    "\n",
    "ConfusionMetrics(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score :  0.2791878172588833\n"
     ]
    }
   ],
   "source": [
    " Compute_F1_Score(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.9377570000000001\n"
     ]
    }
   ],
   "source": [
    "Compute_AUC_Score(dataframe1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute Accuracy Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9718811881188119\n"
     ]
    }
   ],
   "source": [
    "Compute_Accuracy_Score(dataframe1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   y      prob\n",
      "0  0  0.458521\n",
      "1  0  0.505037\n",
      "2  0  0.418652\n",
      "3  0  0.412057\n",
      "4  0  0.375579\n",
      "5  0  0.595387\n",
      "6  0  0.370288\n",
      "7  0  0.299273\n",
      "8  0  0.297000\n",
      "9  0  0.266479\n",
      "\n",
      "\n",
      " 0    1805\n",
      "1    1047\n",
      "Name: y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataframe2 = pd.read_csv(\"5_c.csv\")\n",
    "print(dataframe2.head(10))\n",
    "print(\"\\n\\n\", dataframe2['y'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute the best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold Value:  0.22987164436159915\n",
      "Minimum value of A: 141000\n"
     ]
    }
   ],
   "source": [
    "def Compute_Theshold(dataframe2):\n",
    "    sorted_dataframe = dataframe2.sort_values(by='prob', ascending = True)\n",
    "    Prob_data = list(dataframe2.prob.unique())\n",
    "    Prob_data.sort(reverse = True)\n",
    "    A = []\n",
    "    for threshold in dataframe2['prob']:\n",
    "        y_predict = []\n",
    "        for value in dataframe2['prob']:\n",
    "            if (value <= threshold):\n",
    "                y_predict.append(0)\n",
    "            else:\n",
    "                y_predict.append(1)\n",
    "        dataframe2['y_predict1'] = y_predict\n",
    "    \n",
    "\n",
    "        FP = ((dataframe2['y']==0) & (dataframe2['y_predict1']==1)).sum()\n",
    "        FN = ((dataframe2['y']==1) & (dataframe2['y_predict1']==0)).sum()\n",
    "        A.append((500 * FN) + (100 * FP))\n",
    "    \n",
    "    min_Index = min(b)\n",
    "    ThresholdIndex = A.index(min_Index)\n",
    "    print('Threshold Value: ',sorted_dataframe['prob'][ThresholdIndex])\n",
    "    print('Minimum value of A:', min_Index)\n",
    "    \n",
    "Compute_Theshold(dataframe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>133.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>148.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172.0</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>162.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0\n",
       "5  133.0  153.0\n",
       "6  148.0  139.0\n",
       "7  172.0  145.0\n",
       "8  153.0  162.0\n",
       "9  162.0  154.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe3 = pd.read_csv(\"5_d.csv\")\n",
    "dataframe3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error :  177.16569974554707\n"
     ]
    }
   ],
   "source": [
    "def Compute_Mean_Square(dataframe3):\n",
    "    MSE = 0\n",
    "    for i in range(len(dataframe3)):\n",
    "        MSE = MSE + (dataframe3['y'][i] - dataframe3['pred'][i])**2\n",
    "    MSE = 1/(len(dataframe3))*MSE\n",
    "    print(\"Mean Square Error : \", MSE)\n",
    "    \n",
    "Compute_Mean_Square(dataframe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Compute MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE : 0.1291202994009687\n"
     ]
    }
   ],
   "source": [
    "def Compute_MAPE(dataframe3):\n",
    "    EI =0\n",
    "    AI =0\n",
    "    for i in range(len(dataframe3)):\n",
    "        EI += abs(dataframe3['y'][i] - dataframe3['pred'][i])\n",
    "        AI += (dataframe3['y'][i])\n",
    "    MAPE = EI / AI\n",
    "    print(\"MAPE :\", MAPE)\n",
    "    \n",
    "Compute_MAPE(dataframe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute R^2 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Square Error Value:  0.9563582786990964\n"
     ]
    }
   ],
   "source": [
    "def Compute_R_Square(dataframe3):\n",
    "    y_bar = 0\n",
    "    SS_Total = 0\n",
    "    SS_Res = 0\n",
    "    y_bar =np.mean(dataframe3['y'])\n",
    "    for i in range(len(dataframe3)):\n",
    "        SS_Total += (dataframe3['y'][i] - y_bar)**2\n",
    "        SS_Res += (dataframe3['y'][i] - dataframe3['pred'][i])**2\n",
    "    R_square = 1 -(SS_Res/SS_Total)\n",
    "\n",
    "    print(\"R Square Error Value: \", R_square)\n",
    "    \n",
    "Compute_R_Square(dataframe3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
